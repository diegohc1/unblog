{
  "hash": "b0e5735f4d5a638453593f13c08a5bed",
  "result": {
    "markdown": "---\ntitle: \"Estimando MCO 'manualmente' en R\"\nauthor: \"Diego Hermoza\"\ndate: \"2022-12-24\"\ncategories: [econometrics, R]\nimage: \"image.jpg\"\ndraft: true\ntoc: true\n---\n\n\nEl método más común para la estimación del modelo de regresión lineal es el de mínimos cuadrados. En esta entrada muestro como construir el estimador de mínimos cuadrados ordinarios (MCO; OLS, por sus siglas en inglés) paso a paso en R. En realidad, dados los supuestos del modelo clásico de regresión lineal, es un procedimiento bastante sencillo. La fórmula para estimar los parámetros es: $\\hat{\\beta} = {(X^{\\prime}X)}^{-1}X^{\\prime}y$, donde $X$ contiene las $n$ observaciones de las $k$ variables independientes, y $y$ a las $n$ observaciones de la variable dependiente. Pero... ¿de dónde sale está formula?\n\n## Derivación del estimador de OLS \n\nConsideremos el siguiente modelo de regresión:\n$$y_i = \\beta_0 + \\beta_1x_{1i}  + ... + \\beta_kx_{ki} + u_i$$ \n\ndonde $u_i$ es el componente no observado (unobserved) o error. La ecuación de la regresión puede escribirse de manera matricial de la siguiente forma:\n\n$$y = X\\beta + u$$ \n\nDefinimos $\\hat{\\beta}$ como el estimador de $\\beta$. \nPor lo tanto, se define la ecuación de regresión muestral:\n\n$$\\hat{y} \\equiv X\\hat{\\beta}$$ \n\nConsideremos $e$, la diferencia entre el valor real de $y$ y el valor estimado de $\\hat{y}$. En ese sentido:  \n\n$$e = y - \\hat{y} = y - X\\hat{\\beta}$$ \n\nEl problema de minimización consiste en elegir un vector de coeficientes $\\hat{\\beta}$ tal, que la suma de cuadrados de los residuales $e_i$ sea el mínimo. De manera matricial:\n\n$$e'e =  (y - X\\hat{\\beta})'(y - X\\hat{\\beta})$$ \n$$e'e =  y'y - \\hat{\\beta}'X'y - y'X\\hat{\\beta} + \\hat{\\beta}'X'X\\hat{\\beta}$$\n$$e'e =  y'y - 2\\hat{\\beta}'X'y + \\hat{\\beta}'X'X\\hat{\\beta}$$\nDerivamos respecto a $\\hat{\\beta}$:\n\n$$\\frac{\\partial e'e}{\\partial \\hat{\\beta}} = - 2X'y + 2X'X\\hat{\\beta} = 0$$ \n$$ X'X\\hat{\\beta} = X'y$$\n$$\\hat{\\beta} = {(X^{\\prime}X)}^{-1}X^{\\prime}y $$\nDe esta manera obtenemos la estimación puntual $\\hat{\\beta}$. Generalmente, la pregunta de interés es si el estimador es estadísticamente diferente de $0$.  En ese sentido, debemos evaluar la hipotesís $H_0: \\hat{\\beta} = 0$. Definimos $V(\\hat{\\beta} \\mid X) \\equiv \\sigma^{2}D$, donde $D \\equiv {(X^{\\prime}X)}^{-1}$ y $V(u\\mid X) = \\sigma^{2}$. Entonces: \n\n$$\\hat{\\beta_j}\\sim N(\\beta_j, \\sigma^{2}D_{jj}) $$\nEn palabras: la distribución del j-ésimo elemento del vector de estimadores, es normal, centrada en su verdadero valor y en el j-ésimo elemento de la matriz de varianzas y covarianzas. Bajo la $H_0: \\hat{\\beta} = 0$:\n\n$$z_j \\equiv \\frac{\\hat{\\beta_j}}{\\sqrt{\\sigma^{2}D_{jj}}} \\sim N(0, 1)$$\n\nSin embargo, hay un problema: $\\sigma^{2}$ no se puede observar! Por lo tanto, lo reemplazamos por su estimador $S^{2} \\equiv e'e/(n-k)$. Este cambio transforma la distribución en la distribución $t$: \n\n$$t_j \\equiv \\frac{\\hat{\\beta_j}}{\\sqrt{S^{2}D_{jj}}} \\sim t_{n-k}$$\nSi $t_j$ es muy grande, entonces se rechaza $H_0$. Es preciso dar cuenta que esta es la intuición general de la estimación mediante MCO, sin embargo no hemos entrado en los teoremas y supuestos que respaldan cada uno de los pasos mencionados. Para un mayor detalle se puede consultar cualquier libro de econometría :).   \n\n## Calcular OLS en R \n\nY bueno, ahora exploremos en R. En primer lugar, establecemos los parámetros a estimar y simulamos los datos. \n\n\n\n```{.r .cell-code}\n# parametros a estimar\nN <- 100\nb0 <- 2 \nb1 <- 3 \nb2 <- 5\nb <- c(b0, b1, b2)\nk <- length(b) #nro de parametros\n\n# simulamos datos\nset.seed(1)\nx1 <- runif(N) \nx2 <- runif(N) \nX <- cbind(1, x1, x2) # vector de 1 para b0\nu <- rnorm(N) # unobserved \n\n# generamos y\ny <- X%*%b + u \n\n# y <- b0 + b1*x1 + b2*x2 + u #con ecuaciones \n```\n\n\nConstruimos el estimador $\\hat{\\beta} = {(X^{\\prime}X)}^{-1}X^{\\prime}y$ 'manualmente': \n\n\n\n```{.r .cell-code}\n# tomar en cuenta: \n# solve(): devuelve la inversa de la matriz\n# t():  devuelve la transpuesta de la matriz\n\nbhat <- solve( t(X) %*% X ) %*% t(X) %*% y\n```\n\n\nObtención de la varianza $S^{2} = e'e/(n-k)$\n\n\n```{.r .cell-code}\ne <- y - X %*% bhat #residuales\ns2 <- 1/(N-k) * as.numeric(t(e)%*%e) #estimador de la varianza\n```\n\n\nCalculamos la matriz de varianzas-covarianzas $S^{2}{(X^{\\prime}X)}^{-1}$ y luego sus errores estándar (raíz diagonal de la matriz)\n\n\n\n```{.r .cell-code}\ncov <- s2*solve(t(X) %*% X)\nse <- sqrt(diag(cov)) # error estandar\nt <- (bhat/se) # estadisticos t\np_val <- 2*pt(-abs(t), df = N-k-1) # p_value del estadistico t\n\n#Tabla de la regresion\ntabla <- as.data.frame(round(cbind(b, se, t, p_val), 4))\ntabla <- setNames(tabla, c(\"coef\", \"se\", \"t\", \"pval\"))\nknitr::kable(tabla)\n```\n\n::: {.cell-output-display}\n|   | coef|     se|       t| pval|\n|:--|----:|------:|-------:|----:|\n|   |    2| 0.2791|  7.1333|    0|\n|x1 |    3| 0.3634|  8.0637|    0|\n|x2 |    5| 0.3578| 14.0148|    0|\n:::\n\n```{.r .cell-code}\n#con función de R (lm)\nm1 <- lm(y~x1+x2)\ntexreg::htmlreg(m1)\n```\n\n<table class=\"texreg\" style=\"margin: 10px auto;border-collapse: collapse;border-spacing: 0px;caption-side: bottom;color: #000000;border-top: 2px solid #000000;\">\n<caption>Statistical models</caption>\n<thead>\n<tr>\n<th style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</th>\n<th style=\"padding-left: 5px;padding-right: 5px;\">Model 1</th>\n</tr>\n</thead>\n<tbody>\n<tr style=\"border-top: 1px solid #000000;\">\n<td style=\"padding-left: 5px;padding-right: 5px;\">(Intercept)</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">1.99<sup>&#42;&#42;&#42;</sup></td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">(0.28)</td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">x1</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">2.93<sup>&#42;&#42;&#42;</sup></td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">(0.36)</td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">x2</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">5.01<sup>&#42;&#42;&#42;</sup></td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">&nbsp;</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">(0.36)</td>\n</tr>\n<tr style=\"border-top: 1px solid #000000;\">\n<td style=\"padding-left: 5px;padding-right: 5px;\">R<sup>2</sup></td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">0.73</td>\n</tr>\n<tr>\n<td style=\"padding-left: 5px;padding-right: 5px;\">Adj. R<sup>2</sup></td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">0.73</td>\n</tr>\n<tr style=\"border-bottom: 2px solid #000000;\">\n<td style=\"padding-left: 5px;padding-right: 5px;\">Num. obs.</td>\n<td style=\"padding-left: 5px;padding-right: 5px;\">100</td>\n</tr>\n</tbody>\n<tfoot>\n<tr>\n<td style=\"font-size: 0.8em;\" colspan=\"2\"><sup>&#42;&#42;&#42;</sup>p &lt; 0.001; <sup>&#42;&#42;</sup>p &lt; 0.01; <sup>&#42;</sup>p &lt; 0.05</td>\n</tr>\n</tfoot>\n</table>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}