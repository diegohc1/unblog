---
title: "Estimando MCO 'manualmente' en R"
author: "Diego Hermoza"
date: "2024-13-01"
categories: [econometrics, R]
image: "fig-mco.png"
draft: false
toc: true
---

El método más común para la estimación del modelo de regresión lineal es el de mínimos cuadrados. En esta entrada muestro como construir el estimador de mínimos cuadrados ordinarios (MCO; OLS, por sus siglas en inglés) paso a paso en R. En realidad, dados los supuestos del modelo clásico de regresión lineal, es un procedimiento bastante sencillo. La fórmula para estimar los parámetros es: $\hat{\beta} = {(X^{\prime}X)}^{-1}X^{\prime}y$, donde $X$ contiene las $n$ observaciones de las $k$ variables independientes, y $y$ a las $n$ observaciones de la variable dependiente. Pero... ¿de dónde sale está formula? y ¿cómo evaluamos su significancia estadística?

## Derivación del estimador de OLS e inferencia

Consideremos el siguiente modelo de regresión:
$$y_i = \beta_0 + \beta_1x_{1i}  + ... + \beta_kx_{ki} + u_i$$ 

donde $u_i$ es el componente no observado (unobserved) o error. La ecuación de la regresión puede escribirse de manera matricial de la siguiente forma:

$$y = X\beta + u$$ 

Definimos $\hat{\beta}$ como el estimador de $\beta$. 
Por lo tanto, se define la ecuación de regresión muestral:

$$\hat{y} \equiv X\hat{\beta}$$ 

Consideremos $e$, la diferencia entre el valor real de $y$ y el valor estimado de $\hat{y}$. En ese sentido:  

$$e = y - \hat{y} = y - X\hat{\beta}$$ 

El problema de minimización consiste en elegir un vector de coeficientes $\hat{\beta}$ tal, que la suma de cuadrados de los residuales $e_i$ sea el mínimo. De manera matricial:

$$e'e =  (y - X\hat{\beta})'(y - X\hat{\beta})$$ 
$$e'e =  y'y - \hat{\beta}'X'y - y'X\hat{\beta} + \hat{\beta}'X'X\hat{\beta}$$
$$e'e =  y'y - 2\hat{\beta}'X'y + \hat{\beta}'X'X\hat{\beta}$$
Derivamos respecto a $\hat{\beta}$:

$$\frac{\partial e'e}{\partial \hat{\beta}} = - 2X'y + 2X'X\hat{\beta} = 0$$ 
$$ X'X\hat{\beta} = X'y$$
$$\hat{\beta} = {(X^{\prime}X)}^{-1}X^{\prime}y $$
De esta manera obtenemos la estimación puntual $\hat{\beta}$. Generalmente, la pregunta de interés es si el estimador es estadísticamente diferente de $0$.  En ese sentido, debemos evaluar la hipotesís $H_0: \hat{\beta} = 0$. Definimos $V(\hat{\beta} \mid X) \equiv \sigma^{2}D$, donde $D \equiv {(X^{\prime}X)}^{-1}$ y $V(u\mid X) = \sigma^{2}$. Entonces: 

$$\hat{\beta_j}\sim N(\beta_j, \sigma^{2}D_{jj}) $$
En palabras: la distribución del j-ésimo elemento del vector de estimadores, es normal, centrada en su verdadero valor y en el j-ésimo elemento de la matriz de varianzas y covarianzas. Bajo la $H_0: \hat{\beta} = 0$:

$$z_j \equiv \frac{\hat{\beta_j}}{\sqrt{\sigma^{2}D_{jj}}} \sim N(0, 1)$$

Sin embargo, hay un problema: $\sigma^{2}$ no se puede observar! Por lo tanto, lo reemplazamos por su estimador $S^{2} \equiv e'e/(n-k)$. Este cambio transforma la distribución en la distribución $t$: 

$$t_j \equiv \frac{\hat{\beta_j}}{\sqrt{S^{2}D_{jj}}} \sim t_{n-k}$$
Si $t_j$ es muy grande, entonces se rechaza $H_0$. Es preciso dar cuenta que esta es la intuición general de la estimación mediante MCO, sin embargo no hemos entrado en los teoremas y supuestos que respaldan cada uno de los pasos mencionados. Para un mayor detalle se puede consultar cualquier libro de econometría :).   

## Calcular OLS en R 

Y bueno, ahora exploremos en R. En primer lugar, establecemos los parámetros a estimar y simulamos los datos. 

```{r results = "asis"}
# parametros a estimar
N <- 100
b0 <- 2 
b1 <- 3 
b2 <- 5
b <- c(b0, b1, b2)
k <- length(b) #nro de parametros

# simulamos datos
set.seed(1)
x1 <- runif(N) 
x2 <- runif(N) 
X <- cbind(1, x1, x2) # vector de 1 para b0
u <- rnorm(N) # unobserved 

# generamos y
y <- X%*%b + u 

# y <- b0 + b1*x1 + b2*x2 + u #con ecuaciones 

```

Construimos el estimador $\hat{\beta} = {(X^{\prime}X)}^{-1}X^{\prime}y$:

```{r results = "asis"}
# tomar en cuenta: 
# solve(): devuelve la inversa de la matriz
# t():  devuelve la transpuesta de la matriz

bhat <- solve( t(X) %*% X ) %*% t(X) %*% y
```

Obtención de la varianza $S^{2} = e'e/(n-k)$:
```{r results = "asis"}
e <- y - X %*% bhat #residuales
s2 <- 1/(N-k) * as.numeric(t(e)%*%e) #estimador de la varianza
```

Calculamos la matriz de varianzas-covarianzas $S^{2}{(X^{\prime}X)}^{-1}$ y luego los errores estándar de cada $\hat{\beta_j}$. Esto último, extrayendo la raíz cuadrada de la diagonal de la matriz.

```{r results = "asis"}
cov <- s2*solve(t(X) %*% X) # matriz de covarianzas
se <- sqrt(diag(cov)) # error estandar
```

Finalmente, calculamos los estadisticos $t$ y valores $p$ asociados. 

```{r results = "asis"}
t <- (bhat/se) # estadisticos t
p_val <- 2*pt(-abs(t), df = N-k-1) # p_value del estadistico t
```

Colocamos todo en una tabla:

```{r results = "asis"}
library(kableExtra)

# tabla de la regresion
tabla <- as.data.frame(round(cbind(bhat, se, t, p_val), 4))
tabla <- setNames(tabla, c("coef", "se", "t", "pval"))

tabla |>
  kbl(caption = "Estimación MCO paso a paso") |>
  kable_classic(full_width = F)  

```

Estimando el modelo con la funcion *lm()* en R:

```{r}
m1 <- lm(y ~ 1 + x1 + x2)
summary(m1)

```

Verificamos que los resultados son los mismos. En la práctica, no hay razón para estimar MCO paso a paso porque R viene con las funciones necesarias incorporadas. Sin embargo, el ejercicio nos permite comprender lo que pasa "entre bambalinas".  

## Miscelánea

El método de mínimos cuadrados es considerado como "el automóvil del análisis estadístico moderno”^[Stephen M. Stigler. "Gauss and the Invention of Least Squares." Ann. Statist. 9 (3) 465 - 474, May, 1981. https://doi.org/10.1214/aos/1176345451]. Pero de “moderno” tal vez no tiene mucho, fue publicado por primera vez en 1805 por el matemático francés Adrien-Marie Legendre. Sin embargo, el matemático alemán Carl Friedrich Gauss habría estado usando el método desde 1795. Particularmente, Gauss alcanzó mayor notoriedad y popularidad por calcular la órbita de Ceres (un planeta que en ese momento se había descubierto) empleando mínimos cuadrados. El detalle es que Gauss ni difundió ni explicó el método hasta años después. Entonces, Legendre, aunque no fue el primero en utilizar el método, si fue el primero en difundirlo y nombrarlo como se le conoce hoy. Imaginen inventar el método fundamental del análisis de regresión en el marco de predecir donde aparecerá un planeta. Y uno aqui...  

Pero... en realidad los softwares no utilizan ${(X^{\prime}X)}^{-1}X^{\prime}y$. Al parecer, invertir matrices es “numéricamente inestable” y computacionalmente ineficiente^[Sosa, W. (2015) El lado oscuro de la econometría. TEMAS Grupo Editorial, Buenos Aires]. Por ejemplo, la función *lm()* realiza una descomposición QR (que?) para estimar $\hat{\beta}$:

```{r} 
qx <- qr(X)
solve.qr(qx, y)
```
Y así, es como puedes empezarle a caer mal al profesor de econometría, ¿por qué tanto énfasis en el método MCO?, ¿por qué tantas demostraciones?, ¿Para que hago este post? Si finalmente es descartado por el software. Quién sabe... Me hubiera encantado saber eso en el pregrado :).  

