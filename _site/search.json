[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Diego Hermoza",
    "section": "",
    "text": "Estimando MCO ‘manualmente’ en R\n\n\n\n\n\n\n\neconometrics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 24, 2022\n\n\nDiego Hermoza\n\n\n\n\n\n\n  \n\n\n\n\n¿Qué tal?\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2022\n\n\nDiego Hermoza\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Diego Hermoza",
    "section": "",
    "text": "Me gusta el fútbol, el Hip Hop y la estadística.  Si el mundo es redondo no sé que es ir hacia adelante."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "¿Qué tal?",
    "section": "",
    "text": "¡Hola!, después de unos (varios) intentos pude publicar esta página web. Si bien mis conocimientos en CSS o HTML son escasos o nulos, pude aprovechar las bondades de Quarto y Netlify. En general, utilizando dichas herramientas, es bastante sencillo e intuitivo generar una página (aunque tengo pendiente seguir explorando las bondades).\nDisfruto aprendiendo sobre el lenguaje de programación R y compartir dichos aprendizajes. Es por eso que hice este blog, además de compartir otros temas de interés."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Estimando MCO ‘manualmente’ en R",
    "section": "",
    "text": "El método más común para la estimación del modelo de regresión lineal es el de mínimos cuadrados. En esta entrada muestro como construir el estimador de mínimos cuadrados ordinarios (MCO; OLS, por sus siglas en inglés) paso a paso en R. En realidad, dados los supuestos del modelo clásico de regresión lineal, es un procedimiento bastante sencillo. La fórmula para estimar los parámetros es: \\(\\hat{\\beta} = {(X^{\\prime}X)}^{-1}X^{\\prime}y\\), donde \\(X\\) contiene las \\(n\\) observaciones de las \\(k\\) variables independientes, y \\(y\\) a las \\(n\\) observaciones de la variable dependiente. Pero… ¿de dónde sale está formula? y ¿cómo evaluamos su significancia estadística?"
  },
  {
    "objectID": "posts/post-with-code/index.html#derivación-del-estimador-de-ols-e-inferencia",
    "href": "posts/post-with-code/index.html#derivación-del-estimador-de-ols-e-inferencia",
    "title": "Estimando MCO ‘manualmente’ en R",
    "section": "Derivación del estimador de OLS e inferencia",
    "text": "Derivación del estimador de OLS e inferencia\nConsideremos el siguiente modelo de regresión: \\[y_i = \\beta_0 + \\beta_1x_{1i}  + ... + \\beta_kx_{ki} + u_i\\]\ndonde \\(u_i\\) es el componente no observado (unobserved) o error. La ecuación de la regresión puede escribirse de manera matricial de la siguiente forma:\n\\[y = X\\beta + u\\]\nDefinimos \\(\\hat{\\beta}\\) como el estimador de \\(\\beta\\). Por lo tanto, se define la ecuación de regresión muestral:\n\\[\\hat{y} \\equiv X\\hat{\\beta}\\]\nConsideremos \\(e\\), la diferencia entre el valor real de \\(y\\) y el valor estimado de \\(\\hat{y}\\). En ese sentido:\n\\[e = y - \\hat{y} = y - X\\hat{\\beta}\\]\nEl problema de minimización consiste en elegir un vector de coeficientes \\(\\hat{\\beta}\\) tal, que la suma de cuadrados de los residuales \\(e_i\\) sea el mínimo. De manera matricial:\n\\[e'e =  (y - X\\hat{\\beta})'(y - X\\hat{\\beta})\\] \\[e'e =  y'y - \\hat{\\beta}'X'y - y'X\\hat{\\beta} + \\hat{\\beta}'X'X\\hat{\\beta}\\] \\[e'e =  y'y - 2\\hat{\\beta}'X'y + \\hat{\\beta}'X'X\\hat{\\beta}\\] Derivamos respecto a \\(\\hat{\\beta}\\):\n\\[\\frac{\\partial e'e}{\\partial \\hat{\\beta}} = - 2X'y + 2X'X\\hat{\\beta} = 0\\] \\[ X'X\\hat{\\beta} = X'y\\] \\[\\hat{\\beta} = {(X^{\\prime}X)}^{-1}X^{\\prime}y \\] De esta manera obtenemos la estimación puntual \\(\\hat{\\beta}\\). Generalmente, la pregunta de interés es si el estimador es estadísticamente diferente de \\(0\\). En ese sentido, debemos evaluar la hipotesís \\(H_0: \\hat{\\beta} = 0\\). Definimos \\(V(\\hat{\\beta} \\mid X) \\equiv \\sigma^{2}D\\), donde \\(D \\equiv {(X^{\\prime}X)}^{-1}\\) y \\(V(u\\mid X) = \\sigma^{2}\\). Entonces:\n\\[\\hat{\\beta_j}\\sim N(\\beta_j, \\sigma^{2}D_{jj}) \\] En palabras: la distribución del j-ésimo elemento del vector de estimadores, es normal, centrada en su verdadero valor y en el j-ésimo elemento de la matriz de varianzas y covarianzas. Bajo la \\(H_0: \\hat{\\beta} = 0\\):\n\\[z_j \\equiv \\frac{\\hat{\\beta_j}}{\\sqrt{\\sigma^{2}D_{jj}}} \\sim N(0, 1)\\]\nSin embargo, hay un problema: \\(\\sigma^{2}\\) no se puede observar! Por lo tanto, lo reemplazamos por su estimador \\(S^{2} \\equiv e'e/(n-k)\\). Este cambio transforma la distribución en la distribución \\(t\\):\n\\[t_j \\equiv \\frac{\\hat{\\beta_j}}{\\sqrt{S^{2}D_{jj}}} \\sim t_{n-k}\\] Si \\(t_j\\) es muy grande, entonces se rechaza \\(H_0\\). Es preciso dar cuenta que esta es la intuición general de la estimación mediante MCO, sin embargo no hemos entrado en los teoremas y supuestos que respaldan cada uno de los pasos mencionados. Para un mayor detalle se puede consultar cualquier libro de econometría :)."
  },
  {
    "objectID": "posts/post-with-code/index.html#calcular-ols-en-r",
    "href": "posts/post-with-code/index.html#calcular-ols-en-r",
    "title": "Estimando MCO ‘manualmente’ en R",
    "section": "Calcular OLS en R",
    "text": "Calcular OLS en R\nY bueno, ahora exploremos en R. En primer lugar, establecemos los parámetros a estimar y simulamos los datos.\n# parametros a estimar\nN &lt;- 100\nb0 &lt;- 2 \nb1 &lt;- 3 \nb2 &lt;- 5\nb &lt;- c(b0, b1, b2)\nk &lt;- length(b) #nro de parametros\n\n# simulamos datos\nset.seed(1)\nx1 &lt;- runif(N) \nx2 &lt;- runif(N) \nX &lt;- cbind(1, x1, x2) # vector de 1 para b0\nu &lt;- rnorm(N) # unobserved \n\n# generamos y\ny &lt;- X%*%b + u \n\n# y &lt;- b0 + b1*x1 + b2*x2 + u #con ecuaciones \nConstruimos el estimador \\(\\hat{\\beta} = {(X^{\\prime}X)}^{-1}X^{\\prime}y\\):\n# tomar en cuenta: \n# solve(): devuelve la inversa de la matriz\n# t():  devuelve la transpuesta de la matriz\n\nbhat &lt;- solve( t(X) %*% X ) %*% t(X) %*% y\nObtención de la varianza \\(S^{2} = e'e/(n-k)\\):\ne &lt;- y - X %*% bhat #residuales\ns2 &lt;- 1/(N-k) * as.numeric(t(e)%*%e) #estimador de la varianza\nCalculamos la matriz de varianzas-covarianzas \\(S^{2}{(X^{\\prime}X)}^{-1}\\) y luego los errores estándar de cada \\(\\hat{\\beta_j}\\). Esto último, extrayendo la raíz cuadrada de la diagonal de la matriz.\ncov &lt;- s2*solve(t(X) %*% X) # matriz de covarianzas\nse &lt;- sqrt(diag(cov)) # error estandar\nFinalmente, calculamos los estadisticos \\(t\\) y valores \\(p\\) asociados.\nt &lt;- (bhat/se) # estadisticos t\np_val &lt;- 2*pt(-abs(t), df = N-k-1) # p_value del estadistico t\nColocamos todo en una tabla:\nlibrary(kableExtra)\n\n# tabla de la regresion\ntabla &lt;- as.data.frame(round(cbind(bhat, se, t, p_val), 4))\ntabla &lt;- setNames(tabla, c(\"coef\", \"se\", \"t\", \"pval\"))\n\ntabla %&gt;%\n  kbl(caption = \"Estimación MCO paso a paso\") %&gt;%\n  kable_classic(full_width = F)  \n\n\nEstimación MCO paso a paso\n\n\n\ncoef\nse\nt\npval\n\n\n\n\n\n1.9906\n0.2791\n7.1333\n0\n\n\nx1\n2.9307\n0.3634\n8.0637\n0\n\n\nx2\n5.0144\n0.3578\n14.0148\n0\n\n\n\n\n\n\nEstimando el modelo con la funcion lm() en R:\n\nm1 &lt;- lm(y ~ 1 + x1 + x2)\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ 1 + x1 + x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8579 -0.6167 -0.1432  0.5352  2.3318 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.9906     0.2791   7.133 1.77e-10 ***\nx1            2.9307     0.3634   8.064 1.96e-12 ***\nx2            5.0144     0.3578  14.015  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9675 on 97 degrees of freedom\nMultiple R-squared:  0.7323,    Adjusted R-squared:  0.7268 \nF-statistic: 132.7 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nVerificamos que los resultados son los mismos. En la práctica, no hay razón para estimar MCO paso a paso porque R viene con las funciones necesarias incorporadas. Sin embargo, el ejercicio nos permite comprender lo que pasa “entre bambalinas”."
  },
  {
    "objectID": "posts/post-with-code/index.html#miscelánea",
    "href": "posts/post-with-code/index.html#miscelánea",
    "title": "Estimando MCO ‘manualmente’ en R",
    "section": "Miscelánea",
    "text": "Miscelánea\nEl método de mínimos cuadrados es considerado como “el automóvil del análisis estadístico moderno”1. Pero de “moderno” tal vez no tiene mucho, fue publicado por primera vez en 1805 por el matemático francés Adrien-Marie Legendre. Sin embargo, el matemático alemán Carl Friedrich Gauss habría estado usando el método desde 1795. Particularmente, Gauss alcanzó mayor notoriedad y popularidad por calcular la órbita de Ceres (un planeta que en ese momento se había descubierto) empleando mínimos cuadrados. El detalle es que Gauss ni difundió ni explicó el método hasta años después. Entonces, Legendre, aunque no fue el primero en utilizar el método, si fue el primero en difundirlo y nombrarlo como se le conoce hoy. Imaginen inventar el método fundamental del análisis de regresión en el marco de predecir donde aparecerá un planeta. Y uno aqui…\nPero… en realidad los softwares no utilizan \\({(X^{\\prime}X)}^{-1}X^{\\prime}y\\). Al parecer, invertir matrices es “numéricamente inestable” y computacionalmente ineficiente2. Por ejemplo, la función lm() realiza una transformación QR (?) para estimar \\(\\hat{\\beta}\\):\n\nqx &lt;- qr(X)\nsolve.qr(qx, y)\n\n       [,1]\n   1.990631\nx1 2.930736\nx2 5.014356\n\n\nY así, es como puedes empezarle a caer mal al profesor de econometría, ¿por qué tanto énfasis en el método MCO?, ¿por qué tantas demostraciones?, ¿Para que hago este post? Si finalmente es descartado por el software. ¿Quién sabe? Me hubiera encantado saber eso en el pregrado :)."
  },
  {
    "objectID": "posts/post-with-code/index.html#footnotes",
    "href": "posts/post-with-code/index.html#footnotes",
    "title": "Estimando MCO ‘manualmente’ en R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStephen M. Stigler. “Gauss and the Invention of Least Squares.” Ann. Statist. 9 (3) 465 - 474, May, 1981. https://doi.org/10.1214/aos/1176345451↩︎\nSosa, W. (2015) El lado oscuro de la econometría. TEMAS Grupo Editorial, Buenos Aires↩︎"
  }
]